{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "In this section we will create a data processing template.\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8 - Get the dataset\n",
    "\n",
    "Arrange all data for example in a CSV file. Here we will use [Data.csv](Data.csv) file.\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 9 - Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"numpy: \n",
    "An array object of arbitrary homogeneous items\n",
    "Fast mathematical operations over arrays\n",
    "Linear Algebra, Fourier Transforms, Random Number Generation\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # drawing plots\n",
    "import pandas as pd  # import and manage datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data.csv\n",
    "\n",
    "- It has 4 columns and 10 rows.\n",
    "- Index no of __Country__ column = 0\n",
    "- Index no of __Age__ column = 1\n",
    "- Index no of __Salary__ column = 2\n",
    "- Index no of __Purchased__ column = 3\n",
    "- Index of _ROW_ starts from _0_ as well (in this file we have rows 0-9).\n",
    "\n",
    "#### X:\n",
    "Column 0, 1, 2 are independent\n",
    "\n",
    "#### y:\n",
    "Column 3 is dependent\n",
    "\n",
    "#### Missing values:\n",
    "r4c2 and r6c1\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 10 - Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "\n",
    "# all rows, all columns except the last column\n",
    "X = dataset.iloc[:, :-1].values\n",
    "\n",
    "# all rows, and only 4nd column (or column with index 3)\n",
    "y = dataset.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "\n",
    "![DatasetImport](Img/L9DatasetImport.JPG)\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iloc Vs loc\n",
    "\n",
    "_iloc_: position based | _loc_: label based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49   NaN\n",
       "48   NaN\n",
       "47   NaN\n",
       "46   NaN\n",
       "45   NaN\n",
       "1    NaN\n",
       "2    NaN\n",
       "3    NaN\n",
       "4    NaN\n",
       "5    NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49   NaN\n",
       "48   NaN\n",
       "47   NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.iloc[:3]  # from row 0 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49   NaN\n",
       "48   NaN\n",
       "47   NaN\n",
       "46   NaN\n",
       "45   NaN\n",
       "1    NaN\n",
       "2    NaN\n",
       "3    NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loc[:3]  # till and including label 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12: Taking care of missing data\n",
    "\n",
    "In our dataset, we have two missing data. So,\n",
    "\n",
    "- we either can delete those rows\n",
    "- or, we can populate those two missing data with mean, median or mode\n",
    "\n",
    "Here we are going to take _mean_ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputer\n",
    "\n",
    "__Definition:__ Imputer(self, missing_values=\"NaN\", strategy=\"mean\", axis=0, verbose=0, copy=True)\n",
    "\n",
    "__Type:__ Class in sklearn.preprocessing.imputation module\n",
    "\n",
    "\n",
    "\n",
    "__missing_values:__ integer or “NaN”, optional (default=”NaN”)\n",
    "- The placeholder for the missing values.\n",
    "- All occurrences of missing_values will be imputed.\n",
    "- For missing values encoded as np.nan, use the string value “NaN”.\n",
    "\n",
    "__strategy:__ string, optional (default=”mean”)\n",
    "The imputation strategy.\n",
    "\n",
    "- If “mean”, then replace missing values using the mean along the axis.\n",
    "- If “median”, then replace missing values using the median along the axis.\n",
    "- If “most_frequent”, then replace missing using the most frequent value along the axis.\n",
    "\n",
    "__axis:__ integer, optional (default=0)\n",
    "The axis along which to impute.\n",
    "\n",
    "- If axis=0, then impute along columns.\n",
    "- If axis=1, then impute along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 13: Encoding categorical data\n",
    "\n",
    "We will encode _countries_:\n",
    "\n",
    "- Since we have three countries, we need 3 columns\n",
    "\n",
    "By encoding, we want to achieve the following result for countries:\n",
    "\n",
    "- Column 0 corresponds to France\n",
    "- Column 1 corresponds to Germany\n",
    "- Column 2 corresponds to Spain\n",
    "\n",
    "![CategoricalDataIntro](Img/CategoricalDataIntro.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.40000000e+01,   7.20000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          2.70000000e+01,   4.80000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          3.00000000e+01,   5.40000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          3.80000000e+01,   6.10000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          4.00000000e+01,   6.37777778e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.50000000e+01,   5.80000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          3.87777778e+01,   5.20000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.80000000e+01,   7.90000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          5.00000000e+01,   8.30000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.70000000e+01,   6.70000000e+04]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X (in plot, format %.0f):\n",
    "\n",
    "![CategoricalDataX](Img/CategoricalData_X.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder\n",
    "\n",
    "- Encode labels with value between 0 and n_classes-1.\n",
    "\n",
    "##### For our dataset, we need only LabelEncoder to encode y (_yes_ and _no_ into _1_ and _0_).\n",
    "\n",
    "#### OneHotEncoder\n",
    "\n",
    "__Definition:__ OneHotEncoder(self, n_values=\"auto\", categorical_features=\"all\", dtype=np.float64, sparse=True, handle_unknown='error')\n",
    "\n",
    "- Encode categorical integer features using a one-hot aka one-of-K scheme.\n",
    "- The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix where each column corresponds to one possible value of one feature. \n",
    "- It is assumed that input features take on values in the range [0, n_values).\n",
    "\n",
    "__categorical_features:__ “all” or array of indices or mask.\n",
    "\n",
    "Specify what features are treated as categorical.\n",
    "\n",
    "- ‘all’ (default): All features are treated as categorical.\n",
    "- array of indices: Array of categorical feature indices.\n",
    "- mask: Array of length n_features and with dtype=bool.\n",
    "\n",
    "Non-categorical features are always stacked to the right of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Categorical Data y](Img/CategoricalData_y.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 14: Splitting the dataset into _training set_ and _test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          4.00000000e+01,   6.37777778e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.70000000e+01,   6.70000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          2.70000000e+01,   4.80000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          3.87777778e+01,   5.20000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.80000000e+01,   7.90000000e+04],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          3.80000000e+01,   6.10000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          4.40000000e+01,   7.20000000e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          3.50000000e+01,   5.80000000e+04]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![X_train](Img/X_train.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          3.00000000e+01,   5.40000000e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          5.00000000e+01,   8.30000000e+04]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![X_test](Img/X_test.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![y_train](Img/y_train.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![y_test](Img/y_test.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 15: Feature scaling\n",
    "\n",
    "### Euclidean distance:\n",
    "\n",
    "![Euclidean distance](Img/Euclidean_distance.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$Euclidean Distance = \\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Math\n",
    "Math(r'Euclidean Distance = \\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, _age_ and _salary_ are not on the same scale. _Age_ will be dominated by _salary_. That's why we need to scale the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$x_{stand} = \\frac{x-x_{mean}}{x_{standard_deviation}}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Math(r'x_{stand} = \\frac{x-x_{mean}}{x_{standard_deviation}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$x_{norm} = \\frac{x-x_{min}}{x_{max}-x_{min}}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Math(r'x_{norm} = \\frac{x-x_{min}}{x_{max}-x_{min}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To center the data (zero mean and unit standard error), we subtract the mean and then divide the result by the standard deviation.\n",
    "\n",
    "x′ = (x−μ)/σ\n",
    "\n",
    "We do that on the training set of data. Then we have to apply the same transformation to our testing set (e.g. in model-selection). But we have to use the same two parameters μ and σ that we used for centering the training set.\n",
    "\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, we can call its transform() method to apply the transformation to a particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set X, but it also returns a transformed X′. Internally, it just calls first fit() and then transform() on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For _categorical data_, we can do _feature scaling_ or not which entirely depends on situation.\n",
    "If we do _feature scaling_, we will lose the information about which country did purchase or not, but everything will be on the same scale.\n",
    "In our example, we did _feature scaling_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X_train fit_transform:__\n",
    "\n",
    "![X_train_fit_transform](Img/X_train_fit_transform.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X_test transform:__\n",
    "\n",
    "![X_test_transform](Img/X_test_transform.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to scale y dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 17: Data preprocessing final template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\nsc_X = StandardScaler()\\nX_train = sc_X.fit_transform(X_train)\\nX_test = sc_X.transform(X_test)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Part 1 - Section 2: Data Preprocessing\n",
    "\n",
    "# Importing the  Libraries\n",
    "\n",
    "import numpy as np  # mathematics\n",
    "import matplotlib.pyplot as plt  # drawing plots\n",
    "import pandas as pd  # import and manage datasets\n",
    "\n",
    "# Importing the datasets\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 3].values\n",
    "\n",
    "# Splitting the dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\"\"\"\n",
    "\n",
    "### If requires, take care of missing data and encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
